---
title: "CAA Workshop 1 Benchmark Category 3 using Rust and Python"
tags:
  - hpc
---

Scientific computing means and implies that the targeted applications are solvable on the fastest computer system available now and possibly tomorrow. In the field of computational fluid dynamics (CFD) the simulation on HPC (high-performance computing) systems easily uses the numerical domain consisting of billions of meshes. The demands of multi-physics multi-scale solutions increases complexity of governing equations and their numerical models. With rising of new paradigms in IT (information technologies) the scientific simulations have adopted artificial intelligence (AI) in its prior tools for analysis, e.g., AI-based analysis towards flow control and design optimization.

For a long time, however, the code implementation in CFD tools has been done with FORTRAN or C languages. It is also on-going truth in the most of the CFD engineering. Of course, the fundamental development of the CFD solvers must be built on the robust and accurate numerical methods formulated with the stable libraries. However, at the beginning of the software engineering (at least for the CFD) far more enthusiastic efforts were the main thrust to realize the current advancement in the various fluid dynamic technologies. Furthermore, the seamless coupling of new software libraries could be realized with the new concepts in the modern programming languages. [Rust](https://www.rust-lang.org/ "Rust Programming Language") is the one designed for performance, memory safety, and concurrency. An easy effort within a couple of hours shows that the Rust programming can achieve a better performance in the numerical simulations of real applications.

![CAA Workshop 1 benchmark problem (category 3)](/assets/images/2021-01-21-1.png "2D wave propagation")

Flux calculation in __Python__ code

```python
def CalculateRHS(self):
    ...
    self.DQ[0,:,:,3:-3] =
          self.ACoeff[1]*self.E[:,:,4:-2]
        + self.ACoeff[2]*self.E[:,:,5:-1]
        + self.ACoeff[3]*self.E[:,:,6:  ]
        - self.ACoeff[1]*self.E[:,:,2:-4]
        - self.ACoeff[2]*self.E[:,:,1:-5]
        - self.ACoeff[3]*self.E[:,:, :-6]
    ...
```

Flux calculation in __Rust__ code
```rust
fn spatial_discretization (dom: &mut Domain) -> String {
    let message: String = format!("calculate flux");
    ...
    dom.fielddata[ii+_dq] =
          dom.fielddata[ii-3+_eflux] *acoeff_array[0]
        + dom.fielddata[ii-2+_eflux] *acoeff_array[1]
        + dom.fielddata[ii-1+_eflux] *acoeff_array[2]
        + dom.fielddata[ii+1+_eflux] *acoeff_array[3]
        + dom.fielddata[ii+2+_eflux] *acoeff_array[4]
        + dom.fielddata[ii+3+_eflux] *acoeff_array[5];
    ...
```
The __Python__ code has no implementation of Residual calculation at the end of time integration. Nevertheless, the runtime is 2 times longer than that of __Rust__. But, to get the same result __Rust__ consumes a lot of code lines in the current raw source. Using `ndarray` it could be enhanced in the main project.

* Comparison of runtime for 1000 iterations (2D domain size 501$\times$501)

  |Language   |Code lines (main kernel) |Run-time|
  |-------|---:|------:|
  |Rust   |363 |21.8 s|
  |Python |149 |44.0 s|

Perhaps it is non-sense to port all the existing CFD libraries to __Rust__ ones. The use of this new programming language has a good performance which can be exploited in the development of new libraries. In the next upcoming phase of software technologies we are supposed to be involved in the transition to a completely new era of scientific computing on HPC systems.
